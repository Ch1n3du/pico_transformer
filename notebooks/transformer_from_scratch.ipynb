{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f01c49da",
   "metadata": {},
   "source": [
    "# Transformers from Scratch\n",
    "\n",
    "To gain a thorough understanding of Transformers, I want to attempt writing one from scratch using PyTorch. I'll be referring to Peter Bloem's [blog post]()."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8b534c5",
   "metadata": {},
   "source": [
    "## Basic self-attention\n",
    "\n",
    "Basic self-attention takes a sequence of input vectors $x_1, x_2, ..., x_t$ and produces a sequence of output vectors $y_1, y_2, ..., y_t$. The vectors all have a dimension of $k$.\n",
    "\n",
    "To produce output vector $yi$, the self attention operation simply takes a *weighted average over all the input vectors:*\n",
    "\n",
    "$$ y_i = \\sum_{j} w_{ij}x_j \\$$\n",
    "\n",
    "Where $j$ indexes over the whole sequence nd the weights sume to $1$ over all $j$. The weight $w_{ij}$ is not a parameter, but  it is *derived* from a function over $x_i$ and $x_j$. The simplest option for this function is the dot product:\n",
    "\n",
    "$$ w'_{ij} = {x_i}^{T}{x_j}$$\n",
    "\n",
    ">**Note:** $x_i$ is the input vector at the same position as the current output vector $y_i$.\n",
    ">For the next output vector, we get an entirely new series of dot products, and a different weighted sum.\n",
    "\n",
    "The dot product gives a value between $[-\\infty, \\infty]$ so we'll use a $softmax$ to map the values to between $[0, 1]$:\n",
    "\n",
    "$$ \n",
    "softmax(w'_{ij}) = \n",
    "\\frac\n",
    "{\\exp(w'_{ij})}\n",
    "{{\\sum}_j \\exp(w'_{ij})}\n",
    "$$\n",
    "\n",
    "And that's the basic operation of self-attention.\n",
    "\n",
    "![A visual illustration of basic self-attention. Note that the $softmax$ operation over the weights is not illustrated](../assets/self-attention.svg)\n",
    "*A visual illustration of basic self-attention. Note that the $softmax$ operation over the weights is not illustrated*\n",
    "\n",
    "## In PyTorch: basic self-attention\n",
    "\n",
    "We'll represent the input, a sequence of $t$ vectors of dimension $k$ as a $t$ by $k$ matrix $X$.\n",
    "Including a minibatch dimension $b$, gives us an input tensor of shape $(b, t, k)$.\n",
    "\n",
    "The set of all raw dot products $w'_{ij}$ forms a matrix, which we can compute simply by multiplying $X$ by it's transpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 2            # b\n",
    "sequence_len = 3          # t\n",
    "input_dimension = 4       # k\n",
    "\n",
    "X = torch.ones((batch_size, sequence_len, input_dimension))\n",
    "\n",
    "# - torch.bmm(input: Tensor, mat2: Tensor) -> Tensor\n",
    "#   Performs a batched matrix multiplication of input and mat2.\n",
    "#   It applies matrix multiplication over batches of matrices.\n",
    "#   input and mat2 must be 3-D tensors each containing the same number of matrices.\n",
    "#\n",
    "#   If input is (b × n × m) and out is (b × m × p) out will be (b × n × p) \n",
    "#   It can be thought of as b((n × m) (m × p)) = (b × n × p).\n",
    "#\n",
    "# - torch.transpose(input: Tensor, dim0: int, dim1: int) -> Tensor \n",
    "#   Returns a transpose of the input tensor.\n",
    "#   The given dimensions dim0 and dim1 are swapped.\n",
    "#   - dim0 is the first dimension to be transposed.\n",
    "#   - dim1 is the second dimension to be transposed.\n",
    "# \n",
    "# We transpose dimension 1 and 2 because we want to transpose\n",
    "# the t by k vector containing the weights for a particular input.\n",
    "# (b, t, k) x (b, k, t)\n",
    "raw_weights =  torch.bmm(X, X.transpose(dim0=1, dim1=2))\n",
    "# softmax(w'_ij)\n",
    "weights = F.softmax(raw_weights, dim=2)\n",
    "\n",
    "# w'_\n",
    "y = torch.bmm(weights, X)\n",
    "\n",
    "# And that's all, two matrix multiplications and one softmax gives\n",
    "# us a basic self-attention."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19898f23",
   "metadata": {},
   "source": [
    "## In PyTorch: Complete Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12db94be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, k, heads=4, mask=False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Assert the embedding dimension needs to be divisible by the number of heads.\n",
    "        assert k % heads == 0\n",
    "\n",
    "        self.k, self.heads = k, heads\n",
    "\n",
    "        # These compute the queries, keys and values for all heads.\n",
    "        self.to_keys    = nn.Linear(k, k, bias=False)\n",
    "        self.to_queries = nn.Linear(k, k, bias=False)\n",
    "        self.to_values  = nn.Linear(k, k, bias=False)\n",
    "\n",
    "        # This will be applie after the multi-head attention operation.\n",
    "        self.unify_heads = nn.Linear(k, k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, sequence_len, input_dimension = x.size()\n",
    "        number_of_heads = self.heads\n",
    "\n",
    "        # This gives us three vector sequences of the full embedding dimension\n",
    "        keys    = self.to_keys(x)\n",
    "        queries = self.to_queries(x)\n",
    "        values  = self.to_values(x)\n",
    "\n",
    "        # The size of the scaled down dimension of the multi-head attention heads.\n",
    "        slice_len = input_dimension // number_of_heads\n",
    "\n",
    "        # Reshape from size (b, t, k) -> (b, t, h, s) \n",
    "        # b - Batch size\n",
    "        # t - Sequence length\n",
    "        # k - Original dimension of input vector.\n",
    "        # h - Number of attention heads.\n",
    "        # s - Dimension of key, query and value for an attention head. (h/k)\n",
    "        keys    = keys.view(batch_size, sequence_len, number_of_heads, slice_len)\n",
    "        queries = queries.view(batch_size, sequence_len, number_of_heads, slice_len)\n",
    "        values = values.view(batch_size, sequence_len, number_of_heads, slice_len)\n",
    "\n",
    "        # To compute the dot products, we fold the heads into the batch dimension.\n",
    "        # This ensures we can use torch.bmm() as before.\n",
    "        # Since the head and batch dimension are not next to each other, we need to transpose before we reshape.\n",
    "        keys    = keys.transpose(1, 2).contiguous().view(batch_size * number_of_heads, sequence_len, slice_len) \n",
    "        queries = queries.transpose(1, 2).contiguous().view(batch_size * number_of_heads, sequence_len, slice_len) \n",
    "        values  = values.transpose(1, 2).contiguous().view(batch_size * number_of_heads, sequence_len, slice_len) \n",
    "\n",
    "        # As before the dot products can be computed in a single matrix multiplication,\n",
    "        # but now between the queries and the keys.\n",
    "        \n",
    "        # Get the dot product of the queries, keys and scale.\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        # -- dot has size (b*h, t, t) containing raw weights.\n",
    "\n",
    "        # Scale the dot product\n",
    "        dot = dot / (input_dimension ** 0.5)\n",
    "\n",
    "        # Normalize\n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        # - dot now contains row-wise normalized weights\n",
    "      \n",
    "        # Apply self-attention to the values.\n",
    "        out = torch.bmm(dot, values).view(batch_size, number_of_heads, sequence_len, slice_len)\n",
    "\n",
    "        # To unify the attention heads, we transpose again, so that the head dimension\n",
    "        # and the embedding dimension are next to each other and reshape to get concatenated\n",
    "        # vectors of dimension e.\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, sequence_len, slice_len*number_of_heads)\n",
    "\n",
    "        return self.unify_heads(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2949f96",
   "metadata": {},
   "source": [
    "## Building Transformers\n",
    "\n",
    "A transformer is not just a self-attention layer, it is an *architectur*. We'll define it as: \n",
    "\n",
    "*Any architecture designed to process a connected set of units-such as the tokens in a sequence of the pixesl in an image-where the only interaction between the units is through self-attention.*\n",
    "\n",
    "As with other mechanisms, like convolutions, a more or less standard approach has emerged for how to\n",
    "build self-attention layers into a larger network.\n",
    "The first step is to wrap the self-attention into a **block** that we can repeat.\n",
    "\n",
    "## The Transformer Block\n",
    "\n",
    "There are some variations on how to build a basic transformer block, but most of them are structured roughly like this:\n",
    "\n",
    "![Transformer Block](../assets/transformer-block.svg)\n",
    "\n",
    "That is, the block applies in sequence:\n",
    "\n",
    "1. A Self-attention layer\n",
    "2. A Layer Normalization\n",
    "3. A feed-forward layer (a single MLP applied independently to each vector)\n",
    "4. Anther Layer Normalization\n",
    "\n",
    "The order isn't set in stone the important thing is to combine self-attention with a local feedforward and to add normalization and residual connections.\n",
    "\n",
    ">**Note:** Normalization and residual connections are standard tricks used to help deep neural networks train faster and more accurately.\n",
    ">The layer normalization is applied over the embedding dimension only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697dea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, k, heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(k=k, heads=heads)\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(k)\n",
    "        self.norm_2 = nn.LayerNorm(k)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(k, 4*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*k, k),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm_1(attended + x)\n",
    "\n",
    "        feedforward = self.ff(x)\n",
    "        return self.norm_2(feedforward + x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pico-transformer-lFWYyi5b-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
